#+TITLE: RabbitMQ Streams Scala Client - TODO
#+AUTHOR: Simon
#+DATE: 2025-12-27

* Things I want done

** DONE Core Protocol Implementation
CLOSED: [2025-12-27 Sat]
- All 30 codec files implemented
- Connection management (handshake, heartbeat)
- Stream operations (create, delete)
- Publishing with confirmations
- Subscribing with credit flow control
- Full message routing

** DONE Actor System
CLOSED: [2025-12-27 Sat]
- ConnectionActor - manages protocol and TCP connection
- PublisherActor - handles publishing with confirm tracking
- SubscriberActor - handles consuming with credit flow
- ConnectionSupervisor - manages connection actors
- PublisherSupervisor - manages publisher actors with per-connection ID allocation
- SubscriberSupervisor - manages subscriber actors with per-connection ID allocation
- RabbitMQSupervisor - top-level supervisor coordinating all three

** DONE Client API
CLOSED: [2025-12-27 Sat]
- RabbitMQClient - clean Future-based API
- Publisher wrapper - publish() returns Future[Unit]
- Subscriber wrapper - requestCredit() and close()
- createConnection, createPublisher, createSubscriber
- createStream, deleteStream
- Automatic actor spawning and supervision

** DONE [#A] Test Full Client API in REPL
CLOSED: [2025-12-28 Sun 18:28]
*** DONE Test createStream/deleteStream
CLOSED: [2025-12-28 Sun 18:28]
*** DONE Test Publisher.publish() with Future handling
CLOSED: [2025-12-28 Sun 18:28]
*** DONE Test multiple publishers/subscribers
CLOSED: [2025-12-28 Sun 18:28]
*** DONE Verify error handling
CLOSED: [2025-12-28 Sun 18:28]

** DONE [4/4] [#A] Parse Individual Messages from Chunks
CLOSED: [2025-12-29 Mon 18:17]
*** DONE Implement ChunkParser object
CLOSED: [2025-12-29 Mon 18:17]
*** DONE Parse chunk.messages byte array into individual messages
CLOSED: [2025-12-29 Mon 18:17]
*** DONE Each message has length prefix + data
CLOSED: [2025-12-29 Mon 18:15]
*** DONE Update messageHandler signature to deliver parsed messages
CLOSED: [2025-12-29 Mon 18:14]
Current: =OsirisChunk => Unit=
Target: =(Long, Array[Byte]) => Unit= (offset, data)

** TODO [0/6] [#B] Migrate to Pekko Streams TCP
*** TODO Replace Connection.scala blocking I/O
*** TODO Use Pekko Streams TCP for non-blocking socket operations
*** TODO Update ConnectionActor to use Tcp().outgoingConnection
*** TODO Remove Future-based frame reading
*** TODO Better backpressure and integration with actor system
*** TODO Production-ready connection management

** TODO [0/4] [#B] Add Pekko Streams Integration
*** TODO Add Source API for consuming
=subscriber.source: Source[Message, NotUsed]=
Integrates with Pekko Streams pipelines
*** TODO Add Sink API for publishing
=publisher.sink: Sink[Array[Byte], Future[Done]]=
Automatic backpressure via mapAsync
*** TODO Keep callback API for simple use cases
*** TODO Provide both APIs for flexibility
** TODO [0/4] [#C] Write Documentation
*** TODO [0/5] Update README
**** TODO Installation instructions
**** TODO Quick start guide
**** TODO API examples (callback and streams)
**** TODO Architecture overview
**** TODO Configuration options
*** TODO Add scaladoc comments to public API
*** TODO Document supervision strategies
*** TODO Add troubleshooting section
** TODO [0/6] [#C] Add Proper Tests
*** TODO Unit tests for codecs
*** TODO Integration tests for actors
*** TODO Test supervision and restart behavior
*** TODO Test error handling
*** TODO Test backpressure and credit flow
*** TODO Use Pekko TestKit

* Future Enhancements

** Additional Features
- Offset tracking and storage (StoreOffset, QueryOffset)
- Super stream support (routing, partitions)
- Stream statistics (StreamStats)
- Publisher sequence recovery (QueryPublisherSequence)
- Single active consumer (ConsumerUpdate)
- Connection pooling in client API
- Automatic reconnection on failure
- Metrics and monitoring hooks

** Code Quality
- Add context.watch for automatic deregistration
- Handle max publisher/subscriber ID limits (255)
- Add validation for stream names
- Better error messages
- Performance optimization
- Memory leak prevention

** Developer Experience
- Add builder pattern for configuration
- Fluent API for creating publishers/subscribers
- Better type safety (phantom types for connection state)
- Compile-time validation where possible

* Known Issues

- Version handling in codecs needs cleanup (hardcoded checks)
- Frame reader uses blocking I/O (migrate to Pekko Streams TCP)
- No automatic cleanup of dead actors (add context.watch)
- No handling of max ID limits (255 publishers/subscribers per connection)
- Message parsing not implemented (raw chunks only)
- No offset tracking or storage
- No connection pooling
- No automatic reconnection

* Architecture Notes

** Current Design
#+BEGIN_SRC
RabbitMQClient (user API)
  └─> RabbitMQSupervisor (root)
       ├─> ConnectionSupervisor (manages connections)
       ├─> PublisherSupervisor (manages publishers, tracks IDs per connection)
       └─> SubscriberSupervisor (manages subscribers, tracks IDs per connection)
            ├─> ConnectionActor (protocol + I/O)
            ├─> PublisherActor (publish logic)
            └─> SubscriberActor (consume logic)
#+END_SRC

** Message Flow
- Publishing: User → Publisher → PublisherActor → ConnectionActor → RabbitMQ
- Consuming: RabbitMQ → ConnectionActor → SubscriberActor → messageHandler → User
- Routing: ConnectionActor uses correlation IDs, publisher IDs, and subscription IDs

** Supervision Strategy
- All actors use restart on failure
- Supervisors track children and manage lifecycle
- Clean separation between transport, protocol, and application layers
